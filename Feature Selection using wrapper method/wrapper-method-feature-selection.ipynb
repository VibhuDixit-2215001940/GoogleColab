{"cells":[{"metadata":{"_uuid":"6b95e1ae9a0e96bd0c8eeea60cd8aadc4ef3148c"},"cell_type":"markdown","source":"# Wrapper Method Feature Selection\n* In this method, a subset of features are selected and train a model using them. Based on the inference that we draw from the previous model, we decide to add or remove features from subset.\n"},{"metadata":{"_uuid":"b468239ae9327e467e39c3585329df3157df7398"},"cell_type":"markdown","source":"**I am going to use House Prices data set for Wrapper Feature Selection. So, lets import the data and pre process it before applying feature selection techniques.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"#Loading House prices dataset.\ndf=pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f14123a30096fe0488d9e93112ec3da68a22c92","collapsed":true},"cell_type":"code","source":"df.shape\n#this dataset contain 1460 rows and 81 columns.","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e68a6648ad8ab42e6bc701ce784e5083ad412fbf","collapsed":true},"cell_type":"code","source":"df.info()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ea24f618f2f743b9ac36c22b821092afd35faa38"},"cell_type":"code","source":"#Feature selection should be done after data preprocessing.\n#Ideally  all the categorical variables should be encoded into numbers, so that we can assess how deterministic they are for target.\n#Currently I will be dealling with numerical columns only.\ncolType = ['int64','float64']\n#Select the columns which are either int64 or float64.\nnumCols=list(df.select_dtypes(include=colType).columns)\n#Assigning numerical columns from df to data variable. We can use the same variable as well.\ndata=df[numCols]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df614faa0ac2233c92d4bd4983a48b6563b3093","collapsed":true},"cell_type":"code","source":"#Lets check the shape.\ndata.shape\n#So there are 38 rows which are numerical.","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db09c34756c5841b6869a40d116e74e4686de0c1","collapsed":true},"cell_type":"code","source":"#Lets split the data in training set and test set.\nX_train,X_test,y_train,y_test=train_test_split(data.drop('SalePrice',axis=1),data['SalePrice'],test_size=.2,random_state=1)\n\nX_train.shape,X_test.shape","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d003066b8ccd77ceb8df3dc33d5d993fa25c72e","collapsed":true},"cell_type":"code","source":"def correlation(dataset,threshold):\n    col_corr=set() # set will contains unique values.\n    corr_matrix=dataset.corr() #finding the correlation between columns.\n    for i in range(len(corr_matrix.columns)): #number of columns\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j])>threshold: #checking the correlation between columns.\n                colName=corr_matrix.columns[i] #getting the column name\n                col_corr.add(colName) #adding the correlated column name heigher than threshold value.\n    return col_corr #returning set of column names\ncol=correlation(X_train,0.8)\nprint('Correlated columns:',col)    \n    ","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b286f05abe15559827d96587dc64fcfedf4ed4a2","collapsed":true},"cell_type":"code","source":"#remove correlated columns\nX_train.drop(columns=col,axis=1,inplace=True)\nX_test.drop(columns=col,axis=1,inplace=True)\n#lets check the shape of training set and test set.\nX_train.shape,X_test.shape","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88d2eee7b04ccda34745580b3b36e1897a985567","collapsed":true},"cell_type":"code","source":"#Filling null values with 0.\nX_train.fillna(0,inplace=True)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"374283322c3386fa339240ad03853bf1992f1957","collapsed":true},"cell_type":"code","source":"#Checking if there is null values.\nX_train.isnull().sum().max()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"0fa405cddfb37847918450b54629a329f0437173"},"cell_type":"markdown","source":"# Forward feature selection"},{"metadata":{"trusted":true,"_uuid":"393f50aa50927609175dba979ab0c45b019af4dd","collapsed":true},"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n#I am going to use RandomForestRegressor algoritham as an estimator. Your can select other regression alogritham as well.\nfrom sklearn.ensemble import RandomForestRegressor\n#k_features=10 (It will get top 10 features best suited for prediction)\n#forward=True (Forward feature selection model)\n#verbose=2 (It will show details output as shown below.)\n#cv=5 (Kfold cross valiation: it will split the training set in 5 set and 4 will be using for training the model and 1 will using as validation)\n#n_jobs=-1 (Number of cores it will use for execution.-1 means it will use all the cores of CPU for execution.)\n#scoring='r2'(R-squared is a statistical measure of how close the data are to the fitted regression line)\nmodel=sfs(RandomForestRegressor(),k_features=10,forward=True,verbose=2,cv=5,n_jobs=-1,scoring='r2')\nmodel.fit(X_train,y_train)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27c46a733928762e2a707de0d87723bf6465ceaf","collapsed":true},"cell_type":"code","source":"#Get the selected feature index.\nmodel.k_feature_idx_","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0a316b9e72cbd7c94b2038df599edbe2ba439bdd","collapsed":true},"cell_type":"code","source":"#Get the column name for the selected feature.\nmodel.k_feature_names_","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"30e9d87d1d239119feaf1e04c9b45f66d8604636"},"cell_type":"markdown","source":"**These are the best suited columns for prediction as per Forward Feature Selection.**"},{"metadata":{"_uuid":"ae500a7f344a268092ca479c0ce02c4bdea02f69"},"cell_type":"markdown","source":"# Backward Feature Selection"},{"metadata":{"_uuid":"3dd64543a6ce9ea67bb559391f53f929f13ccde3"},"cell_type":"markdown","source":"**We will be using the same training data for Backward Feature Selection**"},{"metadata":{"trusted":true,"_uuid":"26cab96455314441766a0160ecbec51d30b1031a","collapsed":true},"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.ensemble import RandomForestRegressor\n#k_features=10 (It will get top 10 features best suited for prediction)\n#forward=False (Backward feature selection model)\n#verbose=2 (It will show details output as shown below.)\n#cv=5 (Kfold cross valiation: it will split the training set in 5 set and 4 will be using for training the model and 1 will using as validation)\n#n_jobs=-1 (Number of cores it will use for execution.-1 means it will use all the cores of CPU for execution.)\n#scoring='r2'(R-squared is a statistical measure of how close the data are to the fitted regression line)\nbackwardModel=sfs(RandomForestRegressor(),k_features=10,forward=False,verbose=2,cv=5,n_jobs=-1,scoring='r2')\n#We will convert our training data into numpy array. If we will not convert it, model is not able to read some of the column names. \nbackwardModel.fit(np.array(X_train),y_train)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a0c5ca80b72033c93f9293e4d3e7349e7d97b53","collapsed":true},"cell_type":"code","source":"#Get the selected feature index.\nbackwardModel.k_feature_idx_","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3adcfb02177817fa7b723c957f7d0ae74aa694a7","collapsed":true},"cell_type":"code","source":"#Get the column name for the selected feature.\nX_train.columns[list(backwardModel.k_feature_idx_)]","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"4de3960c1bbaff7dca2748196918c59bb69ba107"},"cell_type":"markdown","source":"# Exhaustive Feature Selection\n In this the best subset of feature is selected, over all possible feature subsets. For example, if a dataset contains 4 features, the algorithm will evaluate all the feature combinations(15) as follows:\n \n Conbination Formula= n!/(r!(n-r)!) where r objects taken from set of n object. \n \n* All possible combinations of 1  feature.  Out of 4 features 1 feature can be selected in 4 different ways.\n* All possible combinations of 2 features. Out of 4 features 2 features can be selected in 6 different ways.\n* All possible combinations of 3 features. Out of 4 features 3 features can be selected in  4 different ways.\n* All possible combinations of 4 features. Out of 4 features 4 features can be selected in  1 way.<br>\nTotal features= 4+6+4+1=15"},{"metadata":{"_uuid":"e4cf492bcc4cc1676095a6b45f3435321b2d6139"},"cell_type":"markdown","source":"**I tried to use same training data for Exhaustive Feature Selection but it is getting stucked since it has 33 features. So we will try to find out top 5 features out of 10 features which we got  from backward feature selection.**"},{"metadata":{"trusted":true,"_uuid":"f834b030374689528e23ef48cc093ed80de7d413","collapsed":true},"cell_type":"code","source":"from mlxtend.feature_selection import ExhaustiveFeatureSelector as efs\n#min_features=1 (minimum number of feature)\n#max_features=5 (maximum number of feature)\n#n_jobs=-1 (Number of cores it will use for execution.-1 means it will use all the cores of CPU for execution.)\n#scoring='r2'(R-squared is a statistical measure of how close the data are to the fitted regression line)\nemodel=efs(RandomForestRegressor(),min_features=1,max_features=5,scoring='r2',n_jobs=-1)\n#Lets take only 10 features which we got from backward feature selection.\nminiData=X_train[X_train.columns[list(backwardModel.k_feature_idx_)]]\n\nemodel.fit(np.array(miniData),y_train)\n#If you see below the model creates 637 feature combinations from 10 features.Thats why its computationally very expensive.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5e1a52a8bf31d7d398a38ae60df7425d51dbb15","collapsed":true},"cell_type":"code","source":"#Get the selected feature index.\nemodel.best_idx_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c71bc332065dd67d38de0328c9019c07094718e","collapsed":true},"cell_type":"code","source":"#Get the column name for the selected feature.\nminiData.columns[list(emodel.best_idx_)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94483f31acf8d59d76e43cb2eed03ea8a76b48bb"},"cell_type":"markdown","source":"**Please checkout [Feature Selection Main Page](https://www.kaggle.com/raviprakash438/feature-selection-technique-in-machine-learning)**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8b76df152958118a6493e912458e7abdf5cbbacc"},"cell_type":"markdown","source":"***Please share your comments,likes or dislikes so that I can improve the post.***"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc9fd9d78563fa1d2666eec1748d6eb584d59bc4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}